{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "os.chdir('/home/ubuntu/F/Jagadeesh/rdsfiles')\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "import pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pyreadr.read_r('bv_property_updated.rds')\n",
    "result = result[None]\n",
    "Final_data= pyreadr.read_r('Final_data_bookings.rds')\n",
    "#Final_data= pyreadr.read_r('Latest_data.rds')\n",
    "Final_data=Final_data[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_data11=Final_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final_data.head()\n",
    "Final_data=Final_data11.copy()\n",
    "import datetime as dt\n",
    "Final_data['bookingdate']=pd.TimedeltaIndex(Final_data['bookingdate'],unit='d')+dt.datetime(1970,1,1)\n",
    "Final_data['arrivaldate']=pd.TimedeltaIndex(Final_data['arrivaldate'],unit='d')+dt.datetime(1970,1,1)\n",
    "Final_data['date']=pd.TimedeltaIndex(Final_data['date'],unit='d')+dt.datetime(1970,1,1)\n",
    "Final_data['departuredate']=pd.TimedeltaIndex(Final_data['departuredate'],unit='d')+dt.datetime(1970,1,1)\n",
    "Final_data=Final_data[['date','house_id','bookingdate', 'arrivaldate', 'departuredate','channel_type','number_of_adults', 'number_of_children', 'number_of_babies','number_of_pets','gross_booking_value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=result[~result.cluster.isna()]\n",
    "valuable_wk_result=result.groupby(['cluster','week'])[['valuable_wk','bedroom_count','number_of_persons']].sum().reset_index()\n",
    "# for i in range(0,52):\n",
    "#     print(result[result.valuable_wk==0])\n",
    "\n",
    "Final_data=Final_data[~Final_data.date.isna()]\n",
    "Final_data['week']=pd.to_numeric(Final_data.date.dt.week.astype('int64'))\n",
    "Final_data['year']=pd.to_numeric(Final_data.date.dt.year.astype('int64'))\n",
    "# Final_data_new=pd.merge(Final_data,result[['house_id','cluster','hol_period','week','country_name']],how='left',on=['house_id','week'])\n",
    "Final_data_new=pd.merge(Final_data,result[['house_id','cluster','hol_period','week']],how='left',on=['house_id','week'])\n",
    "Final_data_new['lead_day']=Final_data_new['arrivaldate']-Final_data_new['bookingdate']\n",
    "Final_data_new['no_days']=Final_data_new['departuredate']-Final_data_new['arrivaldate']\n",
    "Final_data_new['gross_booking_value']=Final_data_new['gross_booking_value']/Final_data_new['no_days'].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclusive_data = result.groupby('cluster')['exclusive_12'].value_counts().unstack().reset_index().fillna(0)\n",
    "exclusive_data = result[['house_id','cluster','exclusive']].drop_duplicates()\n",
    "\n",
    "# Exclusive data\n",
    "exclusive_data = exclusive_data.groupby('cluster')['exclusive'].value_counts().unstack().reset_index().fillna(0)\n",
    "exclusive_data = exclusive_data[['cluster','Exclusive','non-Exclusive']] \n",
    "exclusive_data['exclusive_perc']=pd.to_numeric(exclusive_data['Exclusive']/(exclusive_data['Exclusive']+exclusive_data['non-Exclusive']))\n",
    "exclusive_data['non-exclusive_perc']=pd.to_numeric(exclusive_data['non-Exclusive']/(exclusive_data['Exclusive']+exclusive_data['non-Exclusive']))\n",
    "exclusive_data = exclusive_data[['cluster','exclusive_perc','non-exclusive_perc']] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_allowed_data = result[['house_id','cluster','pets_allowed']].drop_duplicates()\n",
    "pets_allowed_data = pets_allowed_data.groupby('cluster')['pets_allowed'].value_counts().unstack().reset_index().fillna(0)\n",
    "pets_allowed_data =  pets_allowed_data[['cluster','N','Y']] \n",
    "#pets_allowed_data.describe()\n",
    "pets_allowed_data['N_perc']=pd.to_numeric(pets_allowed_data['N']/(pets_allowed_data['N']+pets_allowed_data['Y']))\n",
    "pets_allowed_data['Y_perc']=pd.to_numeric(pets_allowed_data['Y']/(pets_allowed_data['N']+pets_allowed_data['Y']))\n",
    "pets_allowed_data =  pets_allowed_data[['cluster','N_perc','Y_perc']] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accomodation_type\n",
    "\n",
    "acco_data = result[['house_id','cluster','accommodation_type']].drop_duplicates()\n",
    "acco_data = acco_data.groupby('cluster')['accommodation_type'].value_counts().unstack().reset_index().fillna(0)\n",
    "acco_data =  acco_data[['cluster','Apartment','Boat','House','Tentlodge','Unknown']] \n",
    "acco_data['Apartment_perc'] = pd.to_numeric(acco_data['Apartment']/(acco_data[['cluster','Apartment','Boat','House','Tentlodge','Unknown']].sum(axis=1))) \n",
    "acco_data['Boat_perc'] = pd.to_numeric(acco_data['Boat']/(acco_data[['cluster','Apartment','Boat','House','Tentlodge','Unknown']].sum(axis=1))) \n",
    "acco_data['House_perc'] = pd.to_numeric(acco_data['House']/(acco_data[['cluster','Apartment','Boat','House','Tentlodge','Unknown']].sum(axis=1))) \n",
    "acco_data['Tentlodge_perc'] = pd.to_numeric(acco_data['Tentlodge']/(acco_data[['cluster','Apartment','Boat','House','Tentlodge','Unknown']].sum(axis=1))) \n",
    "acco_data['Unknown_perc'] = pd.to_numeric(acco_data['Unknown']/(acco_data[['cluster','Apartment','Boat','House','Tentlodge','Unknown']].sum(axis=1))) \n",
    "acco_data = acco_data[['cluster','Apartment_perc','Boat_perc','House_perc','Tentlodge_perc','Unknown_perc']]\n",
    "#acco_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_data_new['urn']=1\n",
    "Final_data_new=Final_data_new[Final_data_new.arrivaldate<'2019-05-01']\n",
    "#Final_data_new['date_unique']=\n",
    "Final_data_new['date_unique']=(Final_data_new['year'].astype('str')+Final_data_new['week'].astype('str')).astype('int64')\n",
    "# Final_data_new['date_unique']=train_data['date_unique'].astype('int64')\n",
    "Urn_cluster=Final_data_new.groupby(['year','week','cluster','date_unique'])['urn'].sum().reset_index()\n",
    "\n",
    "forecasting_weeks=9\n",
    "predict_log=(forecasting_weeks*7)\n",
    "Final_data_new['lead_day']=Final_data_new['lead_day'].dt.days\n",
    "Final_data_new['Indirect']=0\n",
    "Final_data_new['Direct']=0\n",
    "Final_data_new.loc[Final_data_new.channel_type=='Indirect','Indirect']=1\n",
    "Final_data_new.loc[Final_data_new.channel_type=='Direct','Direct']=1\n",
    "temp_df=Final_data_new[Final_data_new['lead_day']>=predict_log]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(Urn_cluster[Urn_cluster.urn.isna()])\n",
    "#Final_data_new[(Final_data_new.cluster==179)& (Final_data_new.week==18)  & (Final_data_new.year==2019) ]\n",
    "# len(Urn_cluster)\n",
    "# Urn_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_created = Final_data.copy()\n",
    "temp_created['lead_day']=temp_created['arrivaldate']-temp_created['bookingdate']\n",
    "temp_created['lead_day'] = temp_created['lead_day'].dt.days\n",
    "temp_created['lead_day']=temp_created.lead_day.astype('int64')\n",
    "temp_created = temp_created[temp_created['lead_day']>=predict_log]\n",
    "temp_created=pd.merge(temp_created,result[['house_id','cluster','hol_period','week']],how='left',on=['house_id','week'])\n",
    "# temp_created.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brn_date(tf,week_bck,week_fwd):\n",
    "    year=tf['year']\n",
    "    week=tf['week']\n",
    "    weeks=week-week_bck+week_fwd\n",
    "    if weeks<=0:\n",
    "        weeks=52+weeks\n",
    "        year=year-1\n",
    "    if weeks>52:\n",
    "        weeks=weeks-52\n",
    "        year=year+1\n",
    "#     print(weeks,year)\n",
    "    return weeks,year\n",
    "\n",
    "### function which returns the prebooked_URNS for the adjacent weeks \n",
    "def bre_leadday(df,predict_log,week_bck=0,week_fwd=0):\n",
    "    predict_log_new=predict_log+(-week_fwd*7)+(week_bck*7)\n",
    "    print(predict_log_new)\n",
    "    temp=df[df['lead_day']>=predict_log_new]\n",
    "    temp=temp.groupby(['year','week','cluster'])['urn'].sum().reset_index()\n",
    "    temp[['week','year']]=temp.apply(brn_date, axis=1, result_type=\"expand\",args=(week_bck,week_fwd,))\n",
    "    pre_urnname='pre_urn_Wb_'+str(week_bck)+'_WF_'+str(week_fwd)\n",
    "    temp.rename(columns={'urn':pre_urnname},inplace=True)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "70\n",
      "49\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "#getting previus weeks and coming weeks urns\n",
    "pre_brn_1wback=bre_leadday(Final_data_new,predict_log,week_bck=0,week_fwd=1)\n",
    "pre_brn_1wfwd=bre_leadday(Final_data_new,predict_log,week_bck=1,week_fwd=0)\n",
    "pre_brn_2wback=bre_leadday(Final_data_new,predict_log,week_bck=0,week_fwd=2)\n",
    "pre_brn_2wfwd=bre_leadday(Final_data_new,predict_log,week_bck=2,week_fwd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "def q2(x):\n",
    "    return x.quantile(0.50)\n",
    "\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "f = {'lead_day': [q1,q2,q3]}\n",
    "temp_created_1=temp_created[['cluster','week','year','lead_day']].groupby(['cluster','week','year']).agg(f).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['ppl']=temp_df[['number_of_adults', 'number_of_children','number_of_babies']].sum(axis=1)\n",
    "\n",
    "# temp_df=temp_df[['cluster','week','year','urn','country_name','hol_period','ppl','Direct','Indirect','number_of_adults', 'number_of_children','number_of_babies', 'number_of_pets','gross_booking_value']]\n",
    "# train_data=temp_df.groupby(['cluster','week','year','country_name','hol_period'])[['urn','ppl','Direct','Indirect','number_of_adults', 'number_of_children','number_of_babies', 'number_of_pets','gross_booking_value']].sum().reset_index()\n",
    "# merger_data=temp_df.groupby(['cluster','week','year','country_name','hol_period'])[['number_of_adults', 'number_of_children','number_of_babies', 'number_of_pets']].sum().reset_index()\n",
    "# train_data=pd.merge(train_data,merger_data[['cluster','week','year','number_of_adults', 'number_of_children','number_of_babies', 'number_of_pets']],how='left',on=['cluster','week','year'])\n",
    "\n",
    "temp_df=temp_df[['cluster','week','year','urn','hol_period','ppl','Direct','Indirect','number_of_adults', 'number_of_children','number_of_babies', 'number_of_pets','gross_booking_value']]\n",
    "train_data=temp_df.groupby(['cluster','week','year','hol_period'])[['urn','ppl','Direct','Indirect','number_of_adults', 'number_of_children','number_of_babies', 'number_of_pets','gross_booking_value']].sum().reset_index()\n",
    "\n",
    "# merger_data=temp_df.groupby(['cluster','week','year','hol_period'])[['number_of_adults', 'number_of_children','number_of_babies', 'number_of_pets']].sum().reset_index()\n",
    "# train_data=pd.merge(train_data,merger_data[['cluster','week','year','number_of_adults','number_of_babies', 'number_of_pets']],how='left',on=['cluster','week','year'])\n",
    "# train_data=pd.merge(train_data,merger_data[['cluster','week','year','number_of_adults', 'number_of_children','number_of_babies', 'number_of_pets']],how='left',on=['cluster','week','year'])\n",
    "# train_data_cl=train_data.copy()\n",
    "# train_data = pd.merge(train_data,temp_created_1,how='left',on=['cluster','week','year'])\n",
    "\n",
    "#train_data=train_data_cl.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>properties_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191</td>\n",
       "      <td>1106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster  properties_count\n",
       "0  60       1300            \n",
       "1  191      1106            \n",
       "2  207      836             \n",
       "3  52       806             \n",
       "4  106      767             "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties_data = result[['house_id','cluster']].drop_duplicates()\n",
    "prop_count = properties_data.cluster.value_counts().reset_index()\n",
    "prop_count.columns = ['cluster','properties_count']\n",
    "prop_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_data,year,week,date_unique,year_ma=3,week_ma=1,week_fw=0):\n",
    "    yrss = [yrs for yrs in range(year-year_ma,year)]\n",
    "    weeks=[-(52+wk) if wk <=0 else wk for wk in range(week-week_ma,week+1+week_fw)]\n",
    "    df=pd.DataFrame()\n",
    "    for a in yrss:\n",
    "        for b in weeks:\n",
    "            if b<0:\n",
    "                b1,a1=-(b),a-1\n",
    "            elif b>52:\n",
    "                b1,a1=b-52,a+1\n",
    "            else:\n",
    "                b1,a1=b,a\n",
    "            temp=train_data[(train_data.year==a1) & (train_data.week==b1)][['cluster','urn']]\n",
    "            df=pd.concat([temp,df])\n",
    "    df.dropna(inplace=True)\n",
    "    df=df.groupby('cluster').urn.mean().reset_index()\n",
    "    df['date_unique']=int(date_unique)\n",
    "    mean_urn_str='pre_mean_urn_y_'+str(year_ma)+'_w_'+str(week_ma)+'f'+str(week_fw)\n",
    "    df.rename(columns={'urn':mean_urn_str},inplace=True)\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluste_meandf=pd.DataFrame()\n",
    "tr_yrs=list(train_data.year.unique())\n",
    "cluste_mean_y_2_w_0=pd.DataFrame()\n",
    "cluste_mean_y_2_w_2=pd.DataFrame()\n",
    "cluste_mean_y_2_w_1_f_1=pd.DataFrame()\n",
    "for yr in tr_yrs:\n",
    "    for wk in list(range(1,53)):\n",
    "        date_unique=str(yr)+str(wk)\n",
    "        rdf=get_data(train_data,yr,wk,date_unique)\n",
    "        cluste_meandf=pd.concat([cluste_meandf,rdf])\n",
    "        rdf_y_2_w_0=get_data(train_data,yr,wk,date_unique,2,0)\n",
    "        cluste_mean_y_2_w_0=pd.concat([cluste_mean_y_2_w_0,rdf_y_2_w_0])\n",
    "        rdf_y_2_w_2=get_data(train_data,yr,wk,date_unique,2,2)\n",
    "        cluste_mean_y_2_w_2=pd.concat([cluste_mean_y_2_w_2,rdf_y_2_w_2])\n",
    "        rdf_y_2_w_1_f_1=get_data(train_data,yr,wk,date_unique,2,2,1)\n",
    "        cluste_mean_y_2_w_1_f_1=pd.concat([cluste_mean_y_2_w_1_f_1,rdf_y_2_w_1_f_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['date_unique']=train_data['year'].astype('str')+train_data['week'].astype('str')\n",
    "train_data['date_unique']=train_data['date_unique'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>week</th>\n",
       "      <th>year</th>\n",
       "      <th>hol_period</th>\n",
       "      <th>urn</th>\n",
       "      <th>ppl</th>\n",
       "      <th>Direct</th>\n",
       "      <th>Indirect</th>\n",
       "      <th>number_of_adults</th>\n",
       "      <th>number_of_babies</th>\n",
       "      <th>number_of_pets</th>\n",
       "      <th>gross_booking_value</th>\n",
       "      <th>date_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>Christmas</td>\n",
       "      <td>4</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1771.571429</td>\n",
       "      <td>20121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>Christmas</td>\n",
       "      <td>32</td>\n",
       "      <td>296.0</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>270</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>12632.000000</td>\n",
       "      <td>20131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>Christmas</td>\n",
       "      <td>37</td>\n",
       "      <td>341.0</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>292</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>14011.928571</td>\n",
       "      <td>20141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>Christmas</td>\n",
       "      <td>12</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4665.714286</td>\n",
       "      <td>20151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>Christmas</td>\n",
       "      <td>24</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7216.928571</td>\n",
       "      <td>20161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster  week  year hol_period  urn    ppl  Direct  Indirect  number_of_adults  number_of_babies  number_of_pets  gross_booking_value  date_unique\n",
       "0  0        1     2012  Christmas  4    36.0   2       2         36                0                 0               1771.571429          20121      \n",
       "1  0        1     2013  Christmas  32   296.0  10      22        270               4                 0               12632.000000         20131      \n",
       "2  0        1     2014  Christmas  37   341.0  5       32        292               8                 12              14011.928571         20141      \n",
       "3  0        1     2015  Christmas  12   92.0   2       10        86                0                 8               4665.714286          20151      \n",
       "4  0        1     2016  Christmas  24   151.0  0       24        151               0                 0               7216.928571          20161      "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cluste_meandf.rename(columns={'mean_urn_y_3_w_1':'pre_mean_urn_y_3_w_1'},inplace=True)\n",
    "# #cluste_meandf.head()\n",
    "train_data.head()\n",
    "# #cluste_mean_y_2_w_0.head()\n",
    "# cluste_mean_y_2_w_0.rename(columns={'mean_urn_y_2_w_0':'pre_mean_urn_y_2_w_0'},inplace=True)\n",
    "# #cluste_mean_y_2_w_2.head()\n",
    "# cluste_mean_y_2_w_0.rename(columns={'mean_urn_y_2_w_2':'pre_mean_urn_y_2_w_2'},inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.rename(columns={'urn':'pre_urn'},inplace=True)\n",
    "train_data=pd.merge(train_data,Urn_cluster,how='left',on=['cluster','week','year','date_unique'])\n",
    "\n",
    "# train_data['per_pre']=(train_data['urn']-train_data['pre_urn'])/train_data['urn']\n",
    "# train_data[train_data.urn>100][['cluster','week','year','pre_urn','urn','per_pre']]\n",
    "# np.corrcoef(train_data['pre_urn'],train_data['urn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(train_data[train_data.pre_urn.isna()])/len(train_data)\n",
    "train_data=pd.merge(train_data,cluste_meandf,how='left',on=['cluster','date_unique'])\n",
    "train_data=pd.merge(train_data,cluste_mean_y_2_w_0,how='left',on=['cluster','date_unique'])\n",
    "train_data=pd.merge(train_data,cluste_mean_y_2_w_2,how='left',on=['cluster','date_unique'])\n",
    "train_data=pd.merge(train_data,cluste_mean_y_2_w_1_f_1,how='left',on=['cluster','date_unique'])\n",
    "train_data=pd.merge(train_data,prop_count,how='left',on=['cluster'])\n",
    "\n",
    "\n",
    "#train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging pre and post weeks pre_urns\n",
    "train_data=pd.merge(train_data,pre_brn_1wback,how='left',on=['cluster','week','year'])\n",
    "train_data=pd.merge(train_data,pre_brn_1wfwd,how='left',on=['cluster','week','year'])\n",
    "train_data=pd.merge(train_data,pre_brn_2wback,how='left',on=['cluster','week','year'])\n",
    "train_data=pd.merge(train_data,pre_brn_2wfwd,how='left',on=['cluster','week','year'])\n",
    "# preclu=[cl for cl in list(train_data.columns) if cl.startswith('pre_urn_W')]\n",
    "# train_data.loc[:,preclu]=train_data.loc[:,preclu].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>week</th>\n",
       "      <th>year</th>\n",
       "      <th>pre_urn</th>\n",
       "      <th>ppl</th>\n",
       "      <th>Direct</th>\n",
       "      <th>Indirect</th>\n",
       "      <th>number_of_adults</th>\n",
       "      <th>number_of_babies</th>\n",
       "      <th>number_of_pets</th>\n",
       "      <th>gross_booking_value</th>\n",
       "      <th>date_unique</th>\n",
       "      <th>urn</th>\n",
       "      <th>pre_mean_urn_y_3_w_1f0</th>\n",
       "      <th>pre_mean_urn_y_2_w_0f0</th>\n",
       "      <th>pre_mean_urn_y_2_w_2f0</th>\n",
       "      <th>pre_mean_urn_y_2_w_2f1</th>\n",
       "      <th>properties_count</th>\n",
       "      <th>pre_urn_Wb_0_WF_1</th>\n",
       "      <th>pre_urn_Wb_1_WF_0</th>\n",
       "      <th>pre_urn_Wb_0_WF_2</th>\n",
       "      <th>pre_urn_Wb_2_WF_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cluster  week  year  pre_urn  ppl  Direct  Indirect  number_of_adults  number_of_babies  number_of_pets  gross_booking_value  date_unique  urn  pre_mean_urn_y_3_w_1f0  pre_mean_urn_y_2_w_0f0  pre_mean_urn_y_2_w_2f0  pre_mean_urn_y_2_w_2f1  properties_count  pre_urn_Wb_0_WF_1  pre_urn_Wb_1_WF_0  pre_urn_Wb_0_WF_2  pre_urn_Wb_2_WF_0\n",
       "count  0.0      0.0   0.0   0.0      0.0  0.0     0.0       0.0               0.0               0.0             0.0                  0.0          0.0  0.0                     0.0                     0.0                     0.0                     0.0               0.0                0.0                0.0                0.0              \n",
       "mean  NaN      NaN   NaN   NaN      NaN  NaN     NaN       NaN               NaN               NaN             NaN                  NaN          NaN  NaN                     NaN                     NaN                     NaN                     NaN               NaN                NaN                NaN                NaN               \n",
       "std   NaN      NaN   NaN   NaN      NaN  NaN     NaN       NaN               NaN               NaN             NaN                  NaN          NaN  NaN                     NaN                     NaN                     NaN                     NaN               NaN                NaN                NaN                NaN               \n",
       "min   NaN      NaN   NaN   NaN      NaN  NaN     NaN       NaN               NaN               NaN             NaN                  NaN          NaN  NaN                     NaN                     NaN                     NaN                     NaN               NaN                NaN                NaN                NaN               \n",
       "25%   NaN      NaN   NaN   NaN      NaN  NaN     NaN       NaN               NaN               NaN             NaN                  NaN          NaN  NaN                     NaN                     NaN                     NaN                     NaN               NaN                NaN                NaN                NaN               \n",
       "50%   NaN      NaN   NaN   NaN      NaN  NaN     NaN       NaN               NaN               NaN             NaN                  NaN          NaN  NaN                     NaN                     NaN                     NaN                     NaN               NaN                NaN                NaN                NaN               \n",
       "75%   NaN      NaN   NaN   NaN      NaN  NaN     NaN       NaN               NaN               NaN             NaN                  NaN          NaN  NaN                     NaN                     NaN                     NaN                     NaN               NaN                NaN                NaN                NaN               \n",
       "max   NaN      NaN   NaN   NaN      NaN  NaN     NaN       NaN               NaN               NaN             NaN                  NaN          NaN  NaN                     NaN                     NaN                     NaN                     NaN               NaN                NaN                NaN                NaN               "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data.pre_urn.isna()].describe()\n",
    "#train_data.describe()\n",
    "#train_data[train_data.pre_urn_Wb_0_WF_1==277]\n",
    "#train_data['date_unique']=train_data['year'].astype('str')+train_data['week'].astype('str')\n",
    "#train_data['date_unique']=train_data['date_unique'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valuable_wk_result.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hol_period\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "os.chdir('/home/ubuntu/F/Jagadeesh/tdata/pick')\n",
    "le = preprocessing.LabelEncoder()\n",
    "dtyps=train_data.dtypes.reset_index().rename({0:'type','index':'clumn'},axis=1)\n",
    "train_datacolumns=dtyps.loc[dtyps['type']=='object'].clumn.to_list()\n",
    "for column_name in train_datacolumns:\n",
    "    try:\n",
    "        le.fit(train_data[column_name])\n",
    "        print(column_name)\n",
    "        train_data[column_name] = le.fit_transform(train_data[column_name])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e,column_name)\n",
    "        train_data[column_name]=train_data[column_name].factorize()[0]\n",
    "        train_data[column_name] = le.fit_transform(train_data[column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  7, 11,  4, 12,  5, 10, 13, 14,  1,  2,  8,  0,  6, 15,  9])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.hol_period.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_data,year,week,date_unique,year_ma=3,week_ma=1,week_fw=0):\n",
    "    yrss = [yrs for yrs in range(year-year_ma,year)]\n",
    "    weeks=[-(52+wk) if wk <=0 else wk for wk in range(week-week_ma,week+1+week_fw)]\n",
    "    df=pd.DataFrame()\n",
    "    for a in yrss:\n",
    "        for b in weeks:\n",
    "            if b<0:\n",
    "                b1,a1=-(b),a-1\n",
    "            elif b>52:\n",
    "                b1,a1=b-52,a+1\n",
    "            else:\n",
    "                b1,a1=b,a\n",
    "            temp=train_data[(train_data.year==a1) & (train_data.week==b1)][['cluster','urn']]\n",
    "            df=pd.concat([temp,df])\n",
    "    df.dropna(inplace=True)\n",
    "    df=df.groupby('cluster').urn.mean().reset_index()\n",
    "    df['date_unique']=int(date_unique)\n",
    "    mean_urn_str='mean_urn_y_'+str(year_ma)+'_w_'+str(week_ma)+'f'+str(week_fw)\n",
    "    df.rename(columns={'urn':mean_urn_str},inplace=True)\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluste_meandf=pd.DataFrame()\n",
    "tr_yrs=list(train_data.year.unique())\n",
    "cluste_mean_y_2_w_0=pd.DataFrame()\n",
    "cluste_mean_y_2_w_2=pd.DataFrame()\n",
    "cluste_mean_y_2_w_1_f_1=pd.DataFrame()\n",
    "for yr in tr_yrs:\n",
    "    for wk in list(range(1,53)):\n",
    "        date_unique=str(yr)+str(wk)\n",
    "        rdf=get_data(train_data,yr,wk,date_unique)\n",
    "        cluste_meandf=pd.concat([cluste_meandf,rdf])\n",
    "        rdf_y_2_w_0=get_data(train_data,yr,wk,date_unique,2,0)\n",
    "        cluste_mean_y_2_w_0=pd.concat([cluste_mean_y_2_w_0,rdf_y_2_w_0])\n",
    "        rdf_y_2_w_2=get_data(train_data,yr,wk,date_unique,2,2)\n",
    "        cluste_mean_y_2_w_2=pd.concat([cluste_mean_y_2_w_2,rdf_y_2_w_2])\n",
    "        rdf_y_2_w_1_f_1=get_data(train_data,yr,wk,date_unique,2,2,1)\n",
    "        cluste_mean_y_2_w_1_f_1=pd.concat([cluste_mean_y_2_w_1_f_1,rdf_y_2_w_1_f_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "preclu=[cl for cl in list(train_data.columns) if cl.startswith('pre_mean_urn')]\n",
    "train_data.loc[:,preclu]=train_data.loc[:,preclu].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.merge(train_data,cluste_meandf,how='left',on=['cluster','date_unique'])\n",
    "train_data=pd.merge(train_data,cluste_mean_y_2_w_0,how='left',on=['cluster','date_unique'])\n",
    "train_data=pd.merge(train_data,cluste_mean_y_2_w_2,how='left',on=['cluster','date_unique'])\n",
    "train_data=pd.merge(train_data,cluste_mean_y_2_w_1_f_1,how='left',on=['cluster','date_unique'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.head()\n",
    "train_data=pd.merge(train_data,valuable_wk_result,how='left',on=['cluster','week'])\n",
    "train_data = pd.merge(train_data,exclusive_data,how='left',on=['cluster'])\n",
    "train_data = pd.merge(train_data,pets_allowed_data,how='left',on=['cluster'])\n",
    "train_data = pd.merge(train_data,acco_data,how='left',on=['cluster'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.isnull().sum()\n",
    "train_data=pd.merge(train_data,prop_count,how='left',on=['cluster'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cluster', 'week', 'year', 'hol_period', 'pre_urn', 'ppl', 'Direct',\n",
       "       'Indirect', 'number_of_adults', 'number_of_babies', 'number_of_pets',\n",
       "       'gross_booking_value', 'date_unique', 'urn', 'pre_mean_urn_y_3_w_1f0',\n",
       "       'pre_mean_urn_y_2_w_0f0', 'pre_mean_urn_y_2_w_2f0',\n",
       "       'pre_mean_urn_y_2_w_2f1', 'properties_count_x', 'pre_urn_Wb_0_WF_1',\n",
       "       'pre_urn_Wb_1_WF_0', 'pre_urn_Wb_0_WF_2', 'pre_urn_Wb_2_WF_0',\n",
       "       'mean_urn_y_3_w_1f0', 'mean_urn_y_2_w_0f0', 'mean_urn_y_2_w_2f0',\n",
       "       'mean_urn_y_2_w_2f1', 'valuable_wk', 'number_of_persons',\n",
       "       'exclusive_perc', 'non-exclusive_perc', 'N_perc', 'Y_perc',\n",
       "       'Apartment_perc', 'Boat_perc', 'House_perc', 'Tentlodge_perc',\n",
       "       'Unknown_perc', 'properties_count_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27136\n",
      "17047\n",
      "30.662313142745433\n"
     ]
    }
   ],
   "source": [
    "# len(train_data)\n",
    "train_data2=train_data.copy()\n",
    "#train_data2.fillna(0,inplace=True)\n",
    "print(len(train_data2))\n",
    "train_data2.dropna(inplace=True)\n",
    "print(len(train_data2))\n",
    "train_data2.sort_values(by=['date_unique'],inplace=True)\n",
    "start=-52\n",
    "end=-3\n",
    "weeks_day=train_data2.date_unique.unique().tolist()[start:end]\n",
    "weeks_day_train=train_data2.date_unique.unique().tolist()[start-forecasting_weeks:end-forecasting_weeks]\n",
    "# train_data2.drop(columns=['number_of_babies', 'number_of_pets'],inplace=True)\n",
    "mean_error=[]\n",
    "Allweeks_result=pd.DataFrame()\n",
    "for week,weektrain in zip(weeks_day,weeks_day_train):\n",
    "    train = train_data2[train_data2['date_unique'] <= weektrain]\n",
    "    val = train_data2[train_data2['date_unique'] == week]\n",
    "    xtr, xts = train.drop(['urn'], axis=1), val.drop(['urn'], axis=1)\n",
    "    ytr, yts = train['urn'].values, val['urn'].values\n",
    "    cluster_id=xts.cluster\n",
    "    week1=xts.week\n",
    "    pre_urn=xts.pre_urn\n",
    "#    mdl =XGBRegressor(max_depth = 8 , n_estimators=90, min_child_weight=4,subsample=0.9\n",
    "#                      ,colsample_bytree = 1,eval_metric = 'rmse',nthread=-1)\n",
    "    mdl = LGBMRegressor(n_estimators=1000, n_jobs=-1, random_state=0)\n",
    "#     lgbmcv = RFECV(estimator=mdl, step=1, cv=5)\n",
    "#     lgbmcv = lgbmcv.fit(xtr,ytr)\n",
    "    mdl.fit(xtr,ytr)\n",
    "    p  = mdl.predict(xts)\n",
    "    erro_result=pd.DataFrame({'cluster':list(cluster_id),'week':list(week1),'pre_urn':list(pre_urn),'urn':list(yts),'predicted':list(p)})\n",
    "    Allweeks_result=pd.concat([Allweeks_result,erro_result])\n",
    "    error = np.sqrt(metrics.mean_squared_error(yts,p))\n",
    "#     print(np.corrcoef(yts,p))\n",
    "#     print('Week %d - Error %.5f' % (week, error))\n",
    "    mean_error.append(error)\n",
    "    \n",
    "print(sum(mean_error)/len(mean_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           cluster         week      pre_urn          urn    predicted        error\n",
      "count  3546.000000  3546.000000  3546.000000  3546.000000  3546.000000  3546.000000\n",
      "mean   138.548223   27.781726    144.957981   206.550479   214.433581   28.802298  \n",
      "std    67.741037    10.810968    269.743476   348.799886   349.130684   71.397723  \n",
      "           cluster         week      pre_urn          urn    predicted        error\n",
      "count  3004.000000  3004.000000  3004.000000  3004.000000  3004.000000  3004.000000\n",
      "mean   141.580559   28.116844    169.631158   241.620506   249.696647   16.415543  \n",
      "std    66.729227    10.791148    286.192630   368.191727   368.420355   18.413521  \n",
      "           cluster         week      pre_urn          urn    predicted        error\n",
      "count  1600.000000  1600.000000  1600.000000  1600.000000  1600.000000  1600.000000\n",
      "mean   153.244375   28.795000    290.168750   408.746250   418.695871   9.207734   \n",
      "std    65.331068    10.156522    349.889498   440.872979   439.522494   8.115289   \n",
      "          cluster        week      pre_urn          urn    predicted       error\n",
      "count  143.000000  143.000000  143.000000   143.000000   143.000000   143.000000\n",
      "mean   168.881119  29.727273   1195.307692  1568.356643  1568.264604  5.495714  \n",
      "std    62.096754   8.444668    483.989207   516.083946   501.746185   4.090749  \n",
      "32.54879782981663 mean error\n",
      "30.662313142745433\n"
     ]
    }
   ],
   "source": [
    "Allweeks_result['error']=(Allweeks_result['urn']-Allweeks_result['predicted'])/Allweeks_result['urn']\n",
    "Allweeks_result['error']=abs(Allweeks_result['error'])*100\n",
    "\n",
    "print(Allweeks_result[Allweeks_result['urn']>0].describe().head(3))\n",
    "print(Allweeks_result[Allweeks_result['urn']>20].describe().head(3))\n",
    "print(Allweeks_result[Allweeks_result['urn']>100].describe().head(3))\n",
    "print(Allweeks_result[Allweeks_result['urn']>1000].describe().head(3))\n",
    "\n",
    "print(np.sqrt(metrics.mean_squared_error(Allweeks_result.urn,Allweeks_result.predicted)),'mean error')\n",
    "print(sum(mean_error)/len(mean_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    train_data2=train_data.copy()\n",
    "#     print(len(train_data2))\n",
    "    train_data2.dropna(inplace=True)\n",
    "#     print(len(train_data2))\n",
    "    train_data2.sort_values(by=['date_unique'],inplace=True)\n",
    "    start=-52\n",
    "    end=-3\n",
    "    weeks_day=train_data2.date_unique.unique().tolist()[start:end]\n",
    "    weeks_day_train=train_data2.date_unique.unique().tolist()[start-forecasting_weeks:end-forecasting_weeks]\n",
    "    # train_data2.drop(columns=['number_of_babies', 'number_of_pets'],inplace=True)\n",
    "    mean_error=[]\n",
    "#     print(mean_error)\n",
    "    Allweeks_result=pd.DataFrame()\n",
    "    for week,weektrain in zip(weeks_day,weeks_day_train):\n",
    "        train = train_data2[train_data2['date_unique'] <= weektrain]\n",
    "        val = train_data2[train_data2['date_unique'] == week]\n",
    "        xtr, xts = train.drop(['urn'], axis=1), val.drop(['urn'], axis=1)\n",
    "        ytr, yts = train['urn'].values, val['urn'].values\n",
    "        cluster_id=xts.cluster\n",
    "        week1=xts.week\n",
    "        pre_urn=xts.pre_urn\n",
    "        mdl =XGBRegressor(max_depth = int(space['max_depth']),n_estimators=200, eval_metric = 'rmse',nthread=-1)\n",
    "#         mdl =XGBRegressor( colsample_bytree = float(space['colsample_bytree']),\n",
    "#             learning_rate =  float(space['learning_rate']),max_depth = int(space['max_depth']),min_child_weight = int(space['min_child_weight']),subsample = float(space['subsample'])\n",
    "#                           ,n_estimators= int(space['n_estimators']), eval_metric = 'rmse',nthread=-1)\n",
    "    #     mdl = LGBMRegressor(n_estimators=1000, n_jobs=-1, random_state=0)\n",
    "    #     lgbmcv = RFECV(estimator=mdl, step=1, cv=5)\n",
    "    #     lgbmcv = lgbmcv.fit(xtr,ytr)\n",
    "        mdl.fit(xtr,ytr)\n",
    "        p  = mdl.predict(xts)\n",
    "        erro_result=pd.DataFrame({'cluster':list(cluster_id),'week':list(week1),'pre_urn':list(pre_urn),'urn':list(yts),'predicted':list(p)})\n",
    "        Allweeks_result=pd.concat([Allweeks_result,erro_result])\n",
    "        error = np.sqrt(metrics.mean_squared_error(yts,p))\n",
    "    #     print(np.corrcoef(yts,p))\n",
    "    #     print('Week %d - Error %.5f' % (week, error))\n",
    "        mean_error.append(error)\n",
    "#         print(mean_error)\n",
    "        loos=sum(mean_error)/len(mean_error)\n",
    "        allparameter=mdl.get_params()\n",
    "#         print(np.sqrt(metrics.mean_squared_error(Allweeks_result.urn,Allweeks_result.predicted)))\n",
    "        \n",
    "        return{'loss':loos,'space': space,'allparameter':allparameter}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [p for p in range(2, 10,2)]\n",
    "min_child_weight=[p for p in range(3, 10,1)]\n",
    "subsample=[p*0.1 for p in range(5, 10,1)]\n",
    "n_estimators=[p for p in range(50, 250,40)]\n",
    "colsample_bytree =[i/10.0 for i in range(4,10)]\n",
    "learning_rate=[0.1,0.2,0.3,0.05,0.08]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [p for p in range(2, 10,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth=[8]\n",
    "# min_child_weight=[4]\n",
    "# subsample=[0.9]\n",
    "# learning_rate=[0.1]\n",
    "# colsample_bytree=[1]\n",
    "# # learning_rate=[0.1,0.2,0.3,0.05,0.08]\n",
    "# n_estimators=[50]\n",
    "\n",
    "\n",
    "import itertools\n",
    "# a = [max_depth,min_child_weight,subsample,learning_rate,n_estimators,colsample_bytree]\n",
    "a=[max_depth]\n",
    "aa=list(itertools.product(*a))\n",
    "len(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "for i in aa:\n",
    "    print(i)\n",
    "    space={'max_depth':str(i[0])}\n",
    "    reres=objective(space)\n",
    "    result.append(reres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "for i in aa:\n",
    "    space={'max_depth':str(i[0]),'min_child_weight':str(i[1]),'subsample':str(i[2])\n",
    "           ,'learning_rate':str(i[3]),'n_estimators':i[4],'colsample_bytree':i[5]}\n",
    "    reres=objective(space)\n",
    "    result.append(reres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da=pd.DataFrame(result)\n",
    "da.sort_values(by='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(mdl.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cal</th>\n",
       "      <th>val</th>\n",
       "      <th>Perc_imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>date_unique</td>\n",
       "      <td>1732</td>\n",
       "      <td>5.773333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>number_of_pets</td>\n",
       "      <td>1599</td>\n",
       "      <td>5.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>number_of_babies</td>\n",
       "      <td>1591</td>\n",
       "      <td>5.303333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pre_urn</td>\n",
       "      <td>1547</td>\n",
       "      <td>5.156667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pre_urn_Wb_0_WF_2</td>\n",
       "      <td>1432</td>\n",
       "      <td>4.773333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pre_urn_Wb_2_WF_0</td>\n",
       "      <td>1403</td>\n",
       "      <td>4.676667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Indirect</td>\n",
       "      <td>1387</td>\n",
       "      <td>4.623333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pre_urn_Wb_0_WF_1</td>\n",
       "      <td>1284</td>\n",
       "      <td>4.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Direct</td>\n",
       "      <td>1264</td>\n",
       "      <td>4.213333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pre_urn_Wb_1_WF_0</td>\n",
       "      <td>1171</td>\n",
       "      <td>3.903333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mean_urn_y_2_w_0f0</td>\n",
       "      <td>1072</td>\n",
       "      <td>3.573333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>valuable_wk</td>\n",
       "      <td>1048</td>\n",
       "      <td>3.493333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pre_mean_urn_y_2_w_0f0</td>\n",
       "      <td>1013</td>\n",
       "      <td>3.376667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>week</td>\n",
       "      <td>927</td>\n",
       "      <td>3.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ppl</td>\n",
       "      <td>845</td>\n",
       "      <td>2.816667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mean_urn_y_3_w_1f0</td>\n",
       "      <td>832</td>\n",
       "      <td>2.773333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pre_mean_urn_y_3_w_1f0</td>\n",
       "      <td>810</td>\n",
       "      <td>2.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean_urn_y_2_w_2f0</td>\n",
       "      <td>805</td>\n",
       "      <td>2.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>number_of_adults</td>\n",
       "      <td>782</td>\n",
       "      <td>2.606667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mean_urn_y_2_w_2f1</td>\n",
       "      <td>776</td>\n",
       "      <td>2.586667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gross_booking_value</td>\n",
       "      <td>748</td>\n",
       "      <td>2.493333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pre_mean_urn_y_2_w_2f0</td>\n",
       "      <td>708</td>\n",
       "      <td>2.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pre_mean_urn_y_2_w_2f1</td>\n",
       "      <td>678</td>\n",
       "      <td>2.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cluster</td>\n",
       "      <td>620</td>\n",
       "      <td>2.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>exclusive_perc</td>\n",
       "      <td>575</td>\n",
       "      <td>1.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>properties_count_x</td>\n",
       "      <td>569</td>\n",
       "      <td>1.896667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Apartment_perc</td>\n",
       "      <td>535</td>\n",
       "      <td>1.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>N_perc</td>\n",
       "      <td>456</td>\n",
       "      <td>1.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>House_perc</td>\n",
       "      <td>433</td>\n",
       "      <td>1.443333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>number_of_persons</td>\n",
       "      <td>429</td>\n",
       "      <td>1.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hol_period</td>\n",
       "      <td>362</td>\n",
       "      <td>1.206667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>year</td>\n",
       "      <td>267</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Unknown_perc</td>\n",
       "      <td>164</td>\n",
       "      <td>0.546667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Tentlodge_perc</td>\n",
       "      <td>83</td>\n",
       "      <td>0.276667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Boat_perc</td>\n",
       "      <td>52</td>\n",
       "      <td>0.173333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Y_perc</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>non-exclusive_perc</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>properties_count_y</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       cal   val  Perc_imp\n",
       "12  date_unique             1732  5.773333\n",
       "10  number_of_pets          1599  5.330000\n",
       "9   number_of_babies        1591  5.303333\n",
       "4   pre_urn                 1547  5.156667\n",
       "20  pre_urn_Wb_0_WF_2       1432  4.773333\n",
       "21  pre_urn_Wb_2_WF_0       1403  4.676667\n",
       "7   Indirect                1387  4.623333\n",
       "18  pre_urn_Wb_0_WF_1       1284  4.280000\n",
       "6   Direct                  1264  4.213333\n",
       "19  pre_urn_Wb_1_WF_0       1171  3.903333\n",
       "23  mean_urn_y_2_w_0f0      1072  3.573333\n",
       "26  valuable_wk             1048  3.493333\n",
       "14  pre_mean_urn_y_2_w_0f0  1013  3.376667\n",
       "1   week                    927   3.090000\n",
       "5   ppl                     845   2.816667\n",
       "22  mean_urn_y_3_w_1f0      832   2.773333\n",
       "13  pre_mean_urn_y_3_w_1f0  810   2.700000\n",
       "24  mean_urn_y_2_w_2f0      805   2.683333\n",
       "8   number_of_adults        782   2.606667\n",
       "25  mean_urn_y_2_w_2f1      776   2.586667\n",
       "11  gross_booking_value     748   2.493333\n",
       "15  pre_mean_urn_y_2_w_2f0  708   2.360000\n",
       "16  pre_mean_urn_y_2_w_2f1  678   2.260000\n",
       "0   cluster                 620   2.066667\n",
       "28  exclusive_perc          575   1.916667\n",
       "17  properties_count_x      569   1.896667\n",
       "32  Apartment_perc          535   1.783333\n",
       "30  N_perc                  456   1.520000\n",
       "34  House_perc              433   1.443333\n",
       "27  number_of_persons       429   1.430000\n",
       "3   hol_period              362   1.206667\n",
       "2   year                    267   0.890000\n",
       "36  Unknown_perc            164   0.546667\n",
       "35  Tentlodge_perc          83    0.276667\n",
       "33  Boat_perc               52    0.173333\n",
       "31  Y_perc                  1     0.003333\n",
       "29  non-exclusive_perc      0     0.000000\n",
       "37  properties_count_y      0     0.000000"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df=pd.DataFrame({'cal':list(xtr.columns),'val':list(mdl.feature_importances_)})\n",
    "# df.sort_values(by='val',ascending=False)\n",
    " df['Perc_imp']=(df.sort_values(by='val',ascending=False).val/ df.sort_values(by='val',ascending=False).val.sum())*100\n",
    "df.sort_values(by='Perc_imp',ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
